{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:01.244602Z",
     "start_time": "2021-02-13T21:53:01.147654Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:01.945667Z",
     "start_time": "2021-02-13T21:53:01.927167Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a DataFrame\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "penschema = StructType([\n",
    "    StructField(\"pix1\",DoubleType(),True),\n",
    "    StructField(\"pix2\",DoubleType(),True),\n",
    "    StructField(\"pix3\",DoubleType(),True),\n",
    "    StructField(\"pix4\",DoubleType(),True),\n",
    "    StructField(\"pix5\",DoubleType(),True),\n",
    "    StructField(\"pix6\",DoubleType(),True),\n",
    "    StructField(\"pix7\",DoubleType(),True),\n",
    "    StructField(\"pix8\",DoubleType(),True),\n",
    "    StructField(\"pix9\",DoubleType(),True),\n",
    "    StructField(\"pix10\",DoubleType(),True),\n",
    "    StructField(\"pix11\",DoubleType(),True),\n",
    "    StructField(\"pix12\",DoubleType(),True),\n",
    "    StructField(\"pix13\",DoubleType(),True),\n",
    "    StructField(\"pix14\",DoubleType(),True),\n",
    "    StructField(\"pix15\",DoubleType(),True),\n",
    "    StructField(\"pix16\",DoubleType(),True),\n",
    "    StructField(\"label\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "dfpen = ss.read.csv(\"../Data/penbased.dat\", samplingRatio=0.3, schema=penschema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataframe into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:02.348098Z",
     "start_time": "2021-02-13T21:53:02.326631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Training and Test data.\n",
    "pendtsets = dfpen.randomSplit([0.8, 0.2], 1)\n",
    "pendttrain = pendtsets[0].cache()\n",
    "pendtvalid = pendtsets[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformer and estimator and add to a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:02.732881Z",
     "start_time": "2021-02-13T21:53:02.726575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformer - Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(inputCols=dfpen.columns[:-1], outputCol=\"features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:02.917209Z",
     "start_time": "2021-02-13T21:53:02.911879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimator - DecisionTreeClassifier which creates a transformer (Decision Tree Classifier model)\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:03.087921Z",
     "start_time": "2021-02-13T21:53:03.083448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[va, dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the training dataset to pipeline and create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:03.638779Z",
     "start_time": "2021-02-13T21:53:03.631737Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator().setMetricName('f1')\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(dt.maxDepth, [5, 10, 15, 20, 25, 30]).build()\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5,\n",
    "                    estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:29.033479Z",
     "start_time": "2021-02-13T21:53:04.016672Z"
    }
   },
   "outputs": [],
   "source": [
    "cvmodel = cv.fit(pendttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model to the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:29.066610Z",
     "start_time": "2021-02-13T21:53:29.035062Z"
    }
   },
   "outputs": [],
   "source": [
    "dtpredicts = cvmodel.bestModel.transform(pendtvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:53:47.198006Z",
     "start_time": "2021-02-13T21:53:47.074801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9483109728201898"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(dtpredicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T22:06:30.545471Z",
     "start_time": "2021-02-13T22:06:30.412283Z"
    }
   },
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
