{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:00.133460Z",
     "start_time": "2021-02-13T18:19:56.842200Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:01.973269Z",
     "start_time": "2021-02-13T18:20:00.135901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[47.0,\n",
       "  100.0,\n",
       "  27.0,\n",
       "  81.0,\n",
       "  57.0,\n",
       "  37.0,\n",
       "  26.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  23.0,\n",
       "  56.0,\n",
       "  53.0,\n",
       "  100.0,\n",
       "  90.0,\n",
       "  40.0,\n",
       "  98.0,\n",
       "  8.0]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data and create an RDD (16 pixels and label)\n",
    "pen_raw = sc.textFile(\"../Data/penbased.dat\", 4)\\\n",
    "            .map(lambda x:  x.split(\", \"))\\\n",
    "            .map(lambda row: [float(x) for x in row])\n",
    "pen_raw.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:03.729617Z",
     "start_time": "2021-02-13T18:20:01.975613Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a DataFrame\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "penschema = StructType([\n",
    "    StructField(\"pix1\",DoubleType(),True),\n",
    "    StructField(\"pix2\",DoubleType(),True),\n",
    "    StructField(\"pix3\",DoubleType(),True),\n",
    "    StructField(\"pix4\",DoubleType(),True),\n",
    "    StructField(\"pix5\",DoubleType(),True),\n",
    "    StructField(\"pix6\",DoubleType(),True),\n",
    "    StructField(\"pix7\",DoubleType(),True),\n",
    "    StructField(\"pix8\",DoubleType(),True),\n",
    "    StructField(\"pix9\",DoubleType(),True),\n",
    "    StructField(\"pix10\",DoubleType(),True),\n",
    "    StructField(\"pix11\",DoubleType(),True),\n",
    "    StructField(\"pix12\",DoubleType(),True),\n",
    "    StructField(\"pix13\",DoubleType(),True),\n",
    "    StructField(\"pix14\",DoubleType(),True),\n",
    "    StructField(\"pix15\",DoubleType(),True),\n",
    "    StructField(\"pix16\",DoubleType(),True),\n",
    "    StructField(\"label\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "dfpen = ss.createDataFrame(pen_raw, penschema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataframe into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:04.028797Z",
     "start_time": "2021-02-13T18:20:03.731029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Training and Test data.\n",
    "pendtsets = dfpen.randomSplit([0.8, 0.2], 1)\n",
    "pendttrain = pendtsets[0].cache()\n",
    "pendtvalid = pendtsets[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformer and estimator and add to a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:04.127874Z",
     "start_time": "2021-02-13T18:20:04.030099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformer - Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=dfpen.columns[0:-1]) #except the last col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:41:16.693254Z",
     "start_time": "2021-02-13T21:41:16.498296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimator - DecisionTreeClassifier which creates a transformer (Decision Tree Classifier model)\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:04.158772Z",
     "start_time": "2021-02-13T18:20:04.156545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[va,dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:04.169346Z",
     "start_time": "2021-02-13T18:20:04.160425Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "#ParamGridBuilder() â€“ combinations of parameters and their values.\n",
    "paramGrid = ParamGridBuilder().addGrid(dt.maxDepth, [5,10,15,20,25,30]).build()\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid.\n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the training dataset to pipeline and create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:46.518368Z",
     "start_time": "2021-02-13T18:20:04.170824Z"
    }
   },
   "outputs": [],
   "source": [
    "cvmodel = cv.fit(pendttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model to the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:46.559687Z",
     "start_time": "2021-02-13T18:20:46.520164Z"
    }
   },
   "outputs": [],
   "source": [
    "dtpredicts = cvmodel.bestModel.transform(pendtvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:46.781849Z",
     "start_time": "2021-02-13T18:20:46.561433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9585890253909967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# expects two input columns: prediction and label.\n",
    "\n",
    "# f1|accuracy(defulat)|weightedPrecision|weightedRecall|weightedTruePositiveRate| \n",
    "# weightedFalsePositiveRate|weightedFMeasure|truePositiveRateByLabel| \n",
    "# falsePositiveRateByLabel|precisionByLabel|recallByLabel|fMeasureByLabel| \n",
    "# logLoss|hammingLoss\n",
    "metric_name = \"f1\"\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\n",
    "metrics.setMetricName(metric_name) \n",
    "\n",
    "metrics.evaluate(dtpredicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T18:20:47.766698Z",
     "start_time": "2021-02-13T18:20:46.783695Z"
    }
   },
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
